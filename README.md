# ğŸŒ¦ï¸ Real-Time Weather Data Pipeline

Bu proje, **Open-Meteo API**'sinden canlÄ± hava durumu verilerini alÄ±p, **Apache Kafka** ile stream eden, **Apache Spark** ile iÅŸleyen ve **PostgreSQL** veritabanÄ±na kaydeden uÃ§tan uca (End-to-End) bir veri mÃ¼hendisliÄŸi projesidir. SonuÃ§lar **Streamlit** ile canlÄ± olarak gÃ¶rselleÅŸtirilmiÅŸtir.

## ğŸ—ï¸ Mimari

![Architecture](https://miro.medium.com/v2/resize:fit:1400/1*J2QSNM8W1A-X1iWq7jQWQA.png)
*(Not: Buraya kendi Ã§izdiÄŸin bir mimari resmi de koyabilirsin, ÅŸimdilik temsilidir)*

Proje ÅŸu adÄ±mlardan oluÅŸur:
1. **Data Ingestion:** Python scripti API'den veriyi Ã§eker ve Kafka Topic'ine gÃ¶nderir.
2. **Message Broker:** Apache Kafka ve Zookeeper, Docker Ã¼zerinde Ã§alÄ±ÅŸarak veriyi taÅŸÄ±r.
3. **Stream Processing:** PySpark (Structured Streaming), veriyi okur, ÅŸemasÄ±nÄ± dÃ¼zenler, Fahrenheit dÃ¶nÃ¼ÅŸÃ¼mÃ¼ yapar.
4. **Storage:** Ä°ÅŸlenen veriler PostgreSQL veritabanÄ±na yazÄ±lÄ±r.
5. **Visualization:** Streamlit dashboard'u veritabanÄ±ndan anlÄ±k veriyi okuyup grafiÄŸe dÃ¶ker.

## ğŸ› ï¸ KullanÄ±lan Teknolojiler

- **Dil:** Python 3.9+
- **Container:** Docker & Docker Compose
- **Streaming:** Apache Kafka, Zookeeper
- **Processing:** Apache Spark (PySpark)
- **Database:** PostgreSQL
- **Visualization:** Streamlit

ğŸ“‚ Proje Dosya YapÄ±sÄ±

â”œâ”€â”€ docker-compose.yml       # AltyapÄ± (Kafka, Zookeeper, Spark, Postgres)
â”œâ”€â”€ producer.py              # Veri Ãœreticisi (API -> Kafka)
â”œâ”€â”€ spark_processor.py       # Veri Ä°ÅŸleyici (Kafka -> Spark -> DB)
â”œâ”€â”€ dashboard.py             # GÃ¶rselleÅŸtirme (DB -> Streamlit)
â”œâ”€â”€ schema.sql               # VeritabanÄ± tablo kurulum kodu
â”œâ”€â”€ requirements.txt         # Gerekli kÃ¼tÃ¼phaneler
â””â”€â”€ README.md                # DokÃ¼mantasyon

## ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

### 1. AltyapÄ±yÄ± AyaÄŸa KaldÄ±r
Docker kurulu olduÄŸundan emin olun ve servisleri baÅŸlatÄ±n:
docker-compose up -d

Gerekli Python KÃ¼tÃ¼phanelerini Kurun

pip install -r requirements.txt

VeritabanÄ± Tablosunu OluÅŸturun
docker exec -it veri-projesi-db psql -U postgres

SQL satÄ±rÄ± aÃ§Ä±ldÄ±ÄŸÄ±nda ÅŸu kodu yapÄ±ÅŸtÄ±rÄ±n:

CREATE TABLE hava_durumu (
    id SERIAL PRIMARY KEY,
    sehir VARCHAR(50),
    sicaklik_c DOUBLE PRECISION,
    sicaklik_f DOUBLE PRECISION,
    kayit_zamani TIMESTAMP
);

(Ã‡Ä±kmak iÃ§in \q yazabilirsiniz)





Terminal 1: Producer (Veri KaynaÄŸÄ±)

python producer.py


Terminal 2: Spark Processor (Ä°ÅŸleme Motoru) Bu komut, Python dosyasÄ±nÄ± Spark konteynerine kopyalar ve gerekli paketlerle (Kafka & Postgres Driver) Ã§alÄ±ÅŸtÄ±rÄ±r:

# 1. DosyayÄ± konteynere kopyala
docker cp spark_processor.py spark-master:/spark_processor.py

# 2. Spark job'unu baÅŸlat
docker exec -it -u 0 spark-master /opt/spark/bin/spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.7.3 \
  /spark_processor.py



  Terminal 3: Dashboard (GÃ¶rselleÅŸtirme)
  streamlit run dashboard.py

TarayÄ±cÄ±nÄ±zda http://localhost:8501 adresine giderek canlÄ± verileri izleyebilirsiniz


ğŸ‘¨â€ğŸ’» Ä°letiÅŸim
GeliÅŸtirici: Melike OÄŸuzalp

LinkedIn:https://www.linkedin.com/in/melikeoguzalp/

GitHub: https://github.com/melikeoguzalp/
